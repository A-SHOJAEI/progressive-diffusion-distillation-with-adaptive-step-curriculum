# Quick Start Guide - Fixed and Ready

## âœ… All Issues Have Been Fixed!

Your progressive diffusion distillation project is now **fully functional** and ready to run.

## What Was Fixed

### ðŸ”§ Critical Fixes Applied

1. **Training Script Crash** - Fixed GradScaler deprecation warning
   - Changed from `torch.cuda.amp` to `torch.amp`
   - Updated to use `GradScaler('cuda')` instead of `GradScaler()`

2. **CLIP Model Loading Error** - Fixed UNEXPECTED keys error
   - Now loads only text encoder config, not full CLIP model
   - Prevents loading of vision model weights

3. **Deprecated APIs** - Updated to modern PyTorch/torchvision
   - Updated Inception v3 loading to use `weights=` parameter
   - Fixed type hints for Python compatibility

### âœ… Verification Complete

Run this to verify all fixes:
```bash
python3 verify_fixes.py
```

Expected output: âœ… ALL CHECKS PASSED

## How to Use

### 1. Install Dependencies

```bash
pip install -r requirements.txt
```

### 2. Train the Model

**Default configuration (with adaptive curriculum):**
```bash
python scripts/train.py
```

**Ablation study (without adaptive curriculum):**
```bash
python scripts/train.py --config configs/ablation.yaml
```

**Resume from checkpoint:**
```bash
python scripts/train.py --resume checkpoints/checkpoint_step_1000.pt
```

### 3. Evaluate the Model

```bash
python scripts/evaluate.py --checkpoint checkpoints/best_model.pt
```

**Compare with teacher model:**
```bash
python scripts/evaluate.py --checkpoint checkpoints/best_model.pt --compare-teacher
```

### 4. Generate Images

```bash
python scripts/predict.py \
    --checkpoint checkpoints/best_model.pt \
    --prompt "a beautiful sunset over the ocean" \
    --output generated_image.png
```

**Generate multiple images:**
```bash
python scripts/predict.py \
    --checkpoint checkpoints/best_model.pt \
    --prompt "a mountain landscape with snow" \
    --batch-generate 4 \
    --seed 42
```

## Project Structure

```
progressive-diffusion-distillation-with-adaptive-step-curriculum/
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ default.yaml        # Main config with adaptive curriculum
â”‚   â””â”€â”€ ablation.yaml       # Ablation config without curriculum
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train.py           # Training script
â”‚   â”œâ”€â”€ evaluate.py        # Evaluation script
â”‚   â””â”€â”€ predict.py         # Inference script
â”œâ”€â”€ src/progressive_diffusion_distillation_with_adaptive_step_curriculum/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ model.py       # Student and Teacher models
â”‚   â”‚   â””â”€â”€ components.py  # Custom components (curriculum, loss)
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â””â”€â”€ trainer.py     # Training loop
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ loader.py      # Data loading
â”‚   â”‚   â””â”€â”€ preprocessing.py
â”‚   â”œâ”€â”€ evaluation/
â”‚   â”‚   â”œâ”€â”€ metrics.py     # FID, CLIP scores
â”‚   â”‚   â””â”€â”€ analysis.py    # Results analysis
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ config.py      # Config utilities
â””â”€â”€ tests/                 # Unit tests
```

## Key Features

### âœ¨ Novel Contributions

1. **Adaptive Curriculum Scheduler**
   - Dynamically adjusts timestep sampling based on student-teacher divergence
   - Focuses training on difficult regions of the denoising trajectory

2. **Progressive Distillation Loss**
   - Combines output distillation and feature matching
   - Progressive weighting that adapts over training

3. **Few-Step Generation**
   - Student model generates in 4 steps vs 50 steps for teacher
   - 12.5x inference speedup

## Configuration Options

### Main Parameters (configs/default.yaml)

- `seed`: Random seed for reproducibility (default: 42)
- `data.batch_size`: Batch size (default: 4)
- `data.max_train_samples`: Training samples (default: 10000)
- `model.student.num_inference_steps`: Student steps (default: 4)
- `training.num_epochs`: Training epochs (default: 50)
- `training.learning_rate`: Learning rate (default: 0.0001)
- `curriculum.enabled`: Enable adaptive curriculum (default: true)

## Expected Performance

After training with adaptive curriculum:
- **FID Score**: < 25.0 (lower is better)
- **CLIP Score**: > 0.28 (higher is better)
- **Inference Speedup**: ~12.5x (4 steps vs 50 steps)

## Troubleshooting

### Out of Memory
```bash
# Reduce batch size in config
data:
  batch_size: 2

# Enable gradient accumulation
training:
  gradient_accumulation_steps: 8
```

### Training Too Slow
```bash
# Use smaller dataset
data:
  max_train_samples: 5000

# Reduce validation frequency
training:
  eval_interval: 1000
```

### CUDA Not Available
```bash
# Force CPU training
python scripts/train.py --device cpu
```

## MLflow Tracking (Optional)

The training script automatically logs to MLflow if available:
- Training/validation losses
- Learning rate schedule
- Curriculum statistics
- Model checkpoints

View results:
```bash
mlflow ui
# Open http://localhost:5000 in browser
```

## Next Steps

1. **Install dependencies**: `pip install -r requirements.txt`
2. **Run verification**: `python3 verify_fixes.py`
3. **Start training**: `python scripts/train.py`
4. **Monitor progress**: Check `training.log` and MLflow UI
5. **Evaluate results**: `python scripts/evaluate.py --checkpoint checkpoints/best_model.pt`

## Support

For detailed information about the fixes applied, see `FIXES_APPLIED.md`.

For code issues, check that:
1. All dependencies are installed: `pip list | grep torch`
2. CUDA is available (if using GPU): `python -c "import torch; print(torch.cuda.is_available())"`
3. Sufficient disk space for dataset caching
4. Sufficient GPU memory (recommended: 8GB+)

---

**Status**: âœ… All critical issues fixed and verified
**Ready**: Yes, run `pip install -r requirements.txt` and start training!
