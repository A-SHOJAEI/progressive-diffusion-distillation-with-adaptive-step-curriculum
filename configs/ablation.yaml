# Ablation configuration: Baseline without adaptive curriculum

# Random seed for reproducibility
seed: 42

# Data configuration
data:
  dataset: "conceptual_captions"
  split_train: "train"
  split_val: "validation"
  image_size: 512
  batch_size: 4
  num_workers: 0  # Set to 0 to avoid multiprocessing issues
  max_train_samples: 10000
  max_val_samples: 1000
  cache_dir: null

# Model configuration
model:
  teacher:
    text_encoder: "openai/clip-vit-base-patch32"
    num_inference_steps: 50
    scheduler_type: "ddim"

  student:
    text_encoder: "openai/clip-vit-base-patch32"
    num_inference_steps: 4
    use_lora: true
    lora_rank: 4

# Training configuration
training:
  num_epochs: 50
  learning_rate: 0.0001
  weight_decay: 0.01
  gradient_accumulation_steps: 4
  max_grad_norm: 1.0
  mixed_precision: true
  scheduler_type: "cosine"
  early_stopping_patience: 10
  log_interval: 50
  eval_interval: 500
  checkpoint_dir: "checkpoints_baseline"

# Distillation loss configuration
distillation:
  output_weight: 1.0
  feature_weight: 0.5
  kl_weight: 0.1
  progressive_schedule: "linear"

# Adaptive curriculum configuration (DISABLED for ablation)
curriculum:
  enabled: false
  num_regions: 3
  update_frequency: 500
  warmup_steps: 1000

# Evaluation configuration
evaluation:
  compute_fid: true
  compute_clip_score: true
  num_samples: 100
  generation_batch_size: 8
